import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
import json # New import

def chunk_text(pages_text: list[str], source_file: str, chunk_size: int = 500, chunk_overlap: int = 100) -> list[dict]:
    """
    Divides a list of page texts into smaller, overlapping chunks using RecursiveCharacterTextSplitter,
    with associated metadata.

    Args:
        pages_text (list[str]): A list where each element is the text content of a page.
                                This list is typically generated by parsing the output of pdf_processor.py.
        source_file (str): The absolute path to the original source PDF file.
        chunk_size (int): The desired maximum size of each text chunk.
        chunk_overlap (int): The number of characters to overlap between consecutive chunks.

    Returns:
        list[dict]: A list of dictionaries, where each dictionary represents a chunk
                    with its content and associated metadata.
    """
    # Create Document objects for each page to preserve page_number and source_file metadata
    documents = []
    for page_num, page_content in enumerate(pages_text):
        if page_content.strip(): # Only create document for non-empty pages
            doc = Document(
                page_content=page_content,
                metadata={
                    "source_file": source_file,
                    "page_number": page_num + 1,  # 1-based page number
                    "chunk_type": "text",
                }
            )
            documents.append(doc)

    if not documents:
        return []

    # Initialize the RecursiveCharacterTextSplitter with explicit separators
    # It tries to split on:
    # 1. Double newline (paragraphs)
    # 2. Single newline (lines)s
    # 3. Lookbehind for a period followed by a space (sentence-aware splitting)
    # 4. Space
    # 5. Empty string (character by character fallback)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len, # Use character length
        is_separator_regex=True, # Set to True because we are using a regex separator "(?<=\. )"
        separators=["\n\n", "\n", r"(?<=\. )", " ", ""], # Explicitly defined separators - FIXED SYNTAXWARNING
    )

    # Split the documents
    # This method propagates the metadata from the original Document objects to the new chunks.
    split_docs = text_splitter.split_documents(documents)

    # Format the output into a list of dictionaries
    chunks = []
    for i, doc in enumerate(split_docs):
        chunk_metadata = doc.metadata.copy()
        chunk_metadata["chunk_id"] = f"chunk_{i}" # Assign a unique ID to each final chunk
        chunks.append({"content": doc.page_content, "metadata": chunk_metadata})

    return chunks

"""Commenting out individual program testing code

if __name__ == "__main__":
    # Example Usage:
    # This section reads the extracted text from the specified file (output of pdf_processor.py)
    # and parses it back into a list of page strings, which is the expected input for chunk_text().
    extracted_text_file_path = os.path.join(
        os.path.dirname(os.path.abspath(__file__)),
        "pdf_files",
        "destination",
        "The-toyota-way-second-edition-chapter_1",
        "The-toyota-way-second-edition-chapter_1_extracted_text.txt"
    )

    pages_text_from_file = []
    current_page_content = []
    try:
        with open(extracted_text_file_path, "r", encoding="utf-8") as f:
            for line in f:
                # Check for page headers to delineate pages
                if line.startswith("--- Page ") and line.strip().endswith(" ---"):
                    if current_page_content:
                        # Join lines to form the content of the previous page
                        pages_text_from_file.append("".join(current_page_content).strip())
                        current_page_content = [] # Reset for the new page
                else:
                    current_page_content.append(line)
            # Add the content of the last page after the loop finishes
            if current_page_content:
                pages_text_from_file.append("".join(current_page_content).strip())
    except FileNotFoundError:
        print(f"Error: Extracted text file not found at {extracted_text_file_path}")
        print("Please ensure pdf_processor.py has been run for 'The-toyota-way-second-edition-chapter_1.pdf'.")
        exit()

    # Define the source file path for metadata
    source_file_path = os.path.join(
        os.path.dirname(os.path.abspath(__file__)),
        "pdf_files",
        "source",
        "The-toyota-way-second-edition-chapter_1.pdf"
    )

    # --- New: Determine PDF-specific output directory for the new file ---
    base_dir = os.path.dirname(os.path.abspath(__file__))
    pdf_destination_base_dir = os.path.join(base_dir, "pdf_files", "destination")
    pdf_output_subfolder_name = os.path.splitext(os.path.basename(source_file_path))[0] # Get name from source_file_path
    pdf_specific_output_dir = os.path.join(pdf_destination_base_dir, pdf_output_subfolder_name)
    # Ensure the directory exists (though pdf_processor.py should have created it)
    os.makedirs(pdf_specific_output_dir, exist_ok=True)

    # --- Change output file name to JSON ---
    output_file_path = os.path.join(pdf_specific_output_dir, "text_chunks.json")

    print("Chunking with RecursiveCharacterTextSplitter (size=500, overlap=100)...")
    chunks = chunk_text(pages_text_from_file, source_file_path)

    # --- Write chunk details to JSON file ---
    if chunks:
        with open(output_file_path, "w", encoding="utf-8") as f_out:
            json.dump(chunks, f_out, indent=4) # Use indent for readability
        print(f"Chunk details written to: {output_file_path}")

        # --- Calculate and print summary statistics ---
        total_chunks = len(chunks)
        chunk_lengths = [len(chunk['content']) for chunk in chunks]
        min_length = min(chunk_lengths)
        max_length = max(chunk_lengths)
        avg_length = sum(chunk_lengths) / total_chunks

        print("\n--- Chunking Summary Statistics ---")
        print(f"Total chunks generated: {total_chunks}")
        print(f"Minimum chunk length: {min_length}")
        print(f"Maximum chunk length: {max_length}")
        print(f"Average chunk length: {avg_length:.2f}")
    else:
        print("No chunks generated.")
"""